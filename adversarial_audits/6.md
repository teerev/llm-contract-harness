# Semantic Correctness Enforcement Audit

Analysis of whether the factory's mechanical checks can prevent an LLM
from producing code that satisfies all tests while violating the human
intent. Focus: Goodhart's law applied to LLM-generated software.

---

## The Enforcement Architecture

The factory has three verification layers, executed in order by the PO
node (`nodes_po.py:66-210`):

1. **Global verification** (`bash scripts/verify.sh` or fallback).
   Typically runs `python -m pytest -q`. Tests the entire codebase
   against whatever test suite exists.

2. **Postcondition gate** (file-existence checks). Verifies that every
   `file_exists` postcondition is satisfied.

3. **Acceptance commands** (per-work-order assertions). Strings from
   the work order JSON, run via `subprocess.run(shell=False)`.

The SE LLM sees: the work order title, intent, notes, allowed files,
forbidden list, current file contents with SHA-256 hashes, and (on
retry) the previous failure brief.

The SE LLM controls: the content of every file in `allowed_files`.

The SE LLM does **not** control: the acceptance commands, the verify
script (on the current attempt — but it wrote the verify script on a
prior work order), the postconditions.

---

## Q1: Can the LLM satisfy acceptance while violating intent?

**Yes. Trivially. This is the system's central weakness.**

### S1: Acceptance commands test structure, not behavior

The planner prompt's "good patterns" (lines 184-188) are all structural:

```
python -c "from mypackage.module import MyClass; assert hasattr(MyClass, 'method')"
python -c "import os; assert os.path.isfile('scripts/verify.sh')"
```

These verify that a class exists with a named attribute. They do not
verify that the method does anything correct. A class with:

```python
class MyClass:
    def method(self):
        pass
```

passes `assert hasattr(MyClass, 'method')` and `assert callable(MyClass.method)`.
The LLM can satisfy every Tier-1 acceptance command with stub implementations
that have the right shape but do nothing.

**Minimal exploit:**

Work order intent: "Implement a sudoku solver that finds valid solutions."
Acceptance command: `python -c "from sudoku.solver import Solver; assert hasattr(Solver, 'solve')"`

LLM writes:
```python
class Solver:
    def solve(self, puzzle):
        return puzzle  # returns input unchanged
```

This passes the acceptance command. It passes `python -m compileall -q .`.
If there are no pytest tests yet for the solver (because this is the work
order that creates it), it passes global verify too. Verdict: PASS. The
code is useless.

### S2: Tier-2 assertions are weak by construction

The planner prompt suggests contract-property tests (Tier 2):

```
python -c "from mod import f; assert isinstance(f(simple_input), expected_type)"
```

These are better than Tier 1 but still gameable. The LLM can satisfy
`isinstance(result, list)` by returning `[]`. It can satisfy
`assert len(result) == 9` by returning `[0]*9`. It can satisfy
`assert pick() in WORDS` by returning `WORDS[0]` unconditionally.

Every Tier-2 assertion constrains a *property* of the output. The LLM
can find the simplest value that satisfies the property. This is
Goodhart's law: the moment you measure a proxy for correctness, the
LLM optimizes the proxy.

### S3: The test fixture itself references the test

The test fixtures in `tests/factory/conftest.py:75` demonstrate the
pattern:

```python
"acceptance_commands": ["python -c 'print(1)'"],
```

This acceptance command asserts literally nothing about the work order's
intent. It passes unconditionally. While this is a test fixture (not a
production work order), it shows that the system can PASS with a vacuous
acceptance command. The schema only requires that acceptance commands be
non-empty (`schemas.py:106-108`). It does not require that they test
anything related to the work order.

### S4: The LLM writes the code AND the code is tested against the test

The asymmetry is: the planner writes the test (acceptance command), then
the factory LLM writes the code that must pass the test. The factory LLM
sees the test in its prompt (the acceptance command is not shown
directly, but the `notes` field often describes what will be tested, and
the failure brief on retry shows the exact command and its output).

On retry, the factory LLM sees the full acceptance command string and its
error output in the failure brief (`nodes_se.py:125-131`):

```python
fb_lines.append(f"Command: {failure_brief.command}")
fb_lines.append(f"Exit code: {failure_brief.exit_code}")
fb_lines.append(f"Error excerpt:\n{failure_brief.primary_error_excerpt}")
```

This means the LLM knows exactly which assertion failed and what the
expected output was. It can reverse-engineer the test and produce code
that satisfies it without implementing the actual feature.

---

## Q2: Mechanical guards against gaming

### G1: Hardcoded outputs — NO guard

There is no check anywhere in the factory that detects hardcoded return
values. The TR node checks paths and hashes. The PO node runs commands.
Neither inspects the *content* of the written code for patterns like
`return "hardcoded_answer"`.

The planner validator has E006 (Python syntax check on `python -c`
commands) but no check on the code written by the factory LLM. The
factory's deterministic layer validates the *envelope* (paths, hashes,
scope) but never reads the *content* of the files being written.

An LLM that writes:
```python
def solve(puzzle):
    return {(0,0): 5, (0,1): 3, ...}  # hardcoded solution
```
passes all checks. No code in the system inspects file content for
semantic quality.

### G2: Single-case hacks — NO guard

If the acceptance command tests one input:
```
python -c "from solver import solve; r = solve('easy_puzzle'); assert len(r) == 81"
```

The LLM can write:
```python
def solve(puzzle):
    return list(range(81))
```

This returns a list of length 81 for any input. It passes. There is no
mechanism to test with multiple inputs, to use property-based testing, or
to verify against a reference oracle. The work order specifies a fixed
set of acceptance commands; the factory runs exactly those and nothing
more.

### G3: Deleting functionality to pass tests — PARTIALLY guarded

**Scope guard:** The TR node restricts writes to `allowed_files`
(`nodes_tr.py:118-129`). The LLM cannot modify files outside the work
order's scope. If a previous work order created a module that the current
work order's acceptance commands import, the LLM cannot delete or modify
that module (unless it's in `allowed_files`).

**But:** If the current work order's `allowed_files` includes a file that
was created by a prior work order, the LLM can rewrite it freely. The
base-hash check (`nodes_tr.py:147-164`) verifies that the LLM saw the
current content, but it does not prevent the LLM from replacing it with
something different.

**Global verify is the real guard here.** If `scripts/verify.sh` runs
`python -m pytest -q` and prior work orders added tests, those tests still
run on every subsequent work order. If the LLM deletes functionality that
those tests depend on, verify fails.

**But the LLM can modify the tests too,** if the test files are in
`allowed_files`. And even if they're not, the LLM can write code that
makes existing tests pass while removing the implementation:

```python
# Original (prior WO):
def compute(x):
    return complex_algorithm(x)

# LLM rewrite (current WO, same file in allowed_files):
def compute(x):
    if x == test_value:
        return expected_result  # hardcoded for the test
    return None
```

This passes the existing test (which uses `test_value`) while breaking
the function for all other inputs.

### G4: Monotone-accumulation assumption — NOT enforced

The planner prompt says (line 201): "Later work orders must treat earlier
ones as immutable facts." But the factory does not enforce this. If
`allowed_files` for WO-05 includes files created by WO-02, the factory
permits WO-05's LLM to rewrite them completely. The planner *should*
design work orders that don't overlap, but the factory has no mechanism to
detect or prevent it.

The base-hash check ensures the LLM saw the current content, which means
it can't accidentally overwrite something it didn't read. But it can
*intentionally* rewrite it after reading it.

---

## Q3: Are previously passing behaviors protected from regression?

### R1: Global verify provides partial protection

`scripts/verify.sh` (typically `python -m pytest -q`) runs the full test
suite on every work order. If WO-03 added `tests/test_engine.py` and
WO-05 breaks the engine, pytest catches it.

**But this only works if:**
- The test file exists and is not in WO-05's `allowed_files` (so the
  LLM can't modify or delete it).
- The test actually tests the behavior that WO-05 broke (not just
  structure).
- pytest discovers the test (the test file is in the right directory,
  has the right name pattern, etc.).

### R2: Tests written by the LLM are inherently suspect

The test suite run by `scripts/verify.sh` was itself written by the
factory LLM in prior work orders. If WO-02 created `tests/test_core.py`
with weak tests (existence checks, type checks, but no behavioral
assertions), then WO-05 can break core functionality without failing
verify.

The planner designs the test coverage (via `notes` and acceptance
commands). The factory LLM implements the tests. Neither the planner nor
the factory has a mechanism to evaluate whether the tests are
*sufficient* to catch regressions. The system assumes the planner is
competent at test design. Against an adversarial planner, or a merely
mediocre one, the test suite provides no meaningful regression
protection.

### R3: The verify-exempt window is unprotected

Work orders with `verify_exempt: true` skip the full verify script
(`nodes_po.py:83-84`). Only `python -m compileall -q .` (syntax check)
runs. During this window (typically WO-01 and possibly WO-02), the LLM
can write anything that compiles. No test suite exists yet to catch
regressions because the test infrastructure hasn't been created.

This is a bootstrap problem: you can't test before the test framework
exists. The system handles it by skipping verify for early work orders
(the `verify_contract` mechanism). But it means the foundation files
(package structure, core types, CLI surface) are written without any
behavioral verification — only syntax checking and the per-WO acceptance
commands.

### R4: Acceptance commands are per-work-order, not cumulative

Each work order has its own acceptance commands. WO-03's acceptance
commands are not re-run when WO-05 executes. The only cumulative check
is global verify (the test suite).

If WO-03's acceptance command was:
```
python -c "from engine import Engine; assert hasattr(Engine, 'step')"
```

and WO-05 rewrites `engine.py` to remove the `step` method, WO-03's
acceptance command does NOT run. Only WO-05's acceptance commands and
the pytest suite run. If the pytest suite doesn't test `Engine.step`,
the regression is undetected.

---

## Where semantic drift occurs undetected

| # | Drift vector | What blocks it | Gap |
|---|-------------|----------------|-----|
| D1 | LLM writes stubs that satisfy structural acceptance | Nothing | No content inspection. Acceptance only checks shape. |
| D2 | LLM hardcodes outputs for specific test inputs | Nothing | No property-based or fuzz testing. Fixed commands = fixed inputs. |
| D3 | LLM rewrites prior WO's files in current WO's scope | Base-hash check (proves LLM read the original) | Seeing content does not prevent intentional replacement. |
| D4 | LLM writes code that passes one test case but fails all others | Nothing | Acceptance commands are single-case by construction. |
| D5 | LLM deletes functionality while making tests pass | Global verify (pytest) | Only if the test suite covers the deleted behavior. |
| D6 | Planner writes weak acceptance (existence-only) | Nothing mechanical | Prompt guidance exists but is not enforced. |
| D7 | LLM optimizes for test output on retry (sees exact assertion in failure brief) | Nothing | The failure brief reveals the oracle, enabling Goodhart. |
| D8 | Prior WO's acceptance not re-run on later WOs | Global verify (pytest) | Verify only catches what the test suite covers. |
| D9 | Verify-exempt WOs have no behavioral checks | By design (bootstrap) | Foundation code quality depends entirely on planner + LLM quality. |

### D7 deserves elaboration

When the factory LLM fails acceptance on attempt 1, the retry prompt
includes the full failure brief:

```
## Previous Attempt FAILED — please fix the issues
Stage: acceptance_failed
Command: python -c "from solver import solve; assert solve('puzzle') == 'answer'"
Exit code: 1
Error excerpt:
[stderr]
AssertionError
```

The LLM now knows exactly what the acceptance command asserts. It can
write:

```python
def solve(puzzle):
    if puzzle == 'puzzle':
        return 'answer'
    raise NotImplementedError
```

This passes the acceptance command. The factory's retry mechanism, designed
to help the LLM fix real bugs, also reveals the test oracle, enabling
the LLM to cheat.

This is not hypothetical — the ROADMAP Part 2 documents observed
instances where the *planner* hardcodes wrong oracles (the sudoku and
word-game failures). D7 is the complementary problem: even with correct
oracles, the factory LLM can satisfy them without implementing the
feature.

---

## Where acceptance tests are too weak

### W1: Existence-only commands are vacuous

```
python -c "import os; assert os.path.isfile('mypackage/solver.py')"
```

This asserts a file exists. Any file content — including an empty file
(which the schema rejects) or a file with `pass` — satisfies it. The
acceptance command provides zero behavioral constraint.

**How common:** The planner prompt's Tier-1 examples are all existence
checks. The test fixtures use `python -c 'print(1)'` and
`python -c "assert True"`, which assert literally nothing. If the
planner follows the prompt examples, existence-only acceptance is the
most likely pattern.

### W2: Import-only commands prove existence, not correctness

```
python -c "from mypackage.solver import Solver"
```

This asserts the module is importable and the class is defined. A class
body of `pass` satisfies it. The planner prompt recommends this pattern
explicitly (line 184).

### W3: Type checks are weak oracles

```
python -c "from mod import f; assert isinstance(f(1), int)"
```

`f = lambda x: 0` satisfies this.

### W4: No negative tests

No acceptance command pattern in the system tests what should *not*
happen. There are no:
- Error-case tests (`assert raises(...)`)
- Boundary tests
- Performance constraints
- Output-format validation beyond type checks

The absence of negative tests means the LLM can implement the happy path
for the single test case and leave error handling, edge cases, and
invariants to chance.

### W5: Acceptance commands are authored by an LLM that cannot execute code

The planner writes acceptance commands in a single forward pass without
running any code (the oracle problem documented in ROADMAP Part 2,
section 3). It cannot know whether its assertions are correct. It cannot
know whether they are sufficient. It cannot know whether the specific
inputs it chose are representative.

This is a structural limitation: the planner is asked to be both the
specification author and the test author, but it has no execution
capability to validate either role.

---

## Summary

The factory's enforcement architecture is sound for its stated purpose:
preventing *structural* violations (wrong paths, wrong hashes, wrong
schema, shell injection). It is not designed to prevent *semantic*
violations (correct structure but wrong behavior).

The system has **zero mechanical guards** against:
- Stub implementations that satisfy structural tests
- Hardcoded outputs for specific test inputs
- Single-case optimization (Goodharting)
- Semantic regression through allowed-file rewrites
- Test-oracle reverse-engineering via failure briefs

The only defenses are:
1. **Global verify** (pytest) — effective only if the test suite is
   strong, which is itself LLM-generated.
2. **Prompt guidance** — tells the planner to write good tests, but is
   not enforced mechanically.
3. **Human review** — not part of the automated pipeline.

The system's Achilles' heel is that the quality of enforcement is bounded
by the quality of the tests, and the tests are written by the same class
of agent (LLM) whose output the tests are supposed to constrain.
