# Factory Retry Logic Audit

Hostile analysis of every failure class, whether retry can change the
outcome, and where the retry logic is wrong.

---

## 1. How Retries Work

There is exactly one retry loop. It lives in the LangGraph routing.

After every failure, control flows: failed node → `_finalize_node` →
`_route_after_finalize`. The router (`graph.py:72-78`) has three exits:

```python
def _route_after_finalize(state: dict) -> str:
    if state.get("verdict") == "PASS":
        return END
    if state["attempt_index"] > state["max_attempts"]:
        return END
    return "se"      # ← unconditional retry
```

The router does not inspect the `failure_brief`. It does not check the
failure `stage`. It does not classify the error. Every non-PASS verdict
that hasn't exhausted attempts gets retried from SE. Period.

The failure brief from the failed attempt is preserved in state
(`graph.py:145`) and injected into the SE retry prompt
(`nodes_se.py:216-218, 121-132`). The retry mechanism's only corrective
signal is the natural-language error excerpt shown to the LLM.

---

## 2. Complete Failure Taxonomy

Every failure in the factory produces a `FailureBrief` with a `stage`
field. The allowed stages are:

```
preflight, exception, llm_output_invalid,
write_scope_violation, stale_context, write_failed,
verify_failed, acceptance_failed
```

Here is each failure class, where it originates, and whether a retry
from SE can logically change the outcome.

### F1: `preflight` — Precondition violation

**Origin:** `nodes_se.py:168-213`

Two sub-cases:
- `file_exists` precondition on a missing file (line 170)
- `file_absent` precondition on an existing file (line 192)

**Can retry change the outcome?** No. The precondition checks the
filesystem. Finalize calls `rollback()` which restores the repo to
baseline. The next SE invocation hits the same precondition on the same
baseline state. The filesystem is identical on every attempt.

The code itself labels these as `"PLANNER-CONTRACT BUG"` and the
`constraints_reminder` says `"This is a plan-level error. The work order
sequence is invalid. Re-run the planner."` — then retries anyway.

**Classification:** *Deterministic failure retried.* Every retry is
guaranteed to produce the same result.

**Wasted compute:** `max_attempts` × (artifact directory creation +
failure_brief write + rollback). No LLM calls are wasted (the LLM is
never reached). But each attempt creates an artifact directory with a
`failure_brief.json`, and rollback executes `git reset --hard` +
`git clean -fdx` — all pointless.

**Correctness risk:** None. No writes are applied. But `run_summary.json`
reports `total_attempts: N` which is misleading — it suggests the system
tried N times when in fact the failure was predetermined.

### F2: `exception` — LLM API call failure

**Origin:** `nodes_se.py:239-247`

The `except Exception` around `llm.complete()` catches everything the
OpenAI SDK or network stack throws: rate limits, timeouts, auth errors,
connection refused, malformed API responses, etc.

**Can retry change the outcome?** Depends on the exception type:

- **Transient (rate limit, timeout, network blip):** Yes. Retry can
  succeed. But there is no backoff. The retry goes through the full
  SE→finalize→rollback→SE cycle, re-reads context files, re-builds the
  prompt, then immediately re-calls the LLM. If the failure was a rate
  limit, the retry hits the same rate limit.

- **Permanent (bad API key, model not found, package not installed):**
  No. `OPENAI_API_KEY` is read from the environment on every call
  (`llm.py:20`). A new `openai.OpenAI()` client is constructed on every
  call (`llm.py:43`). The same bad key or missing package will fail
  identically every time.

**Classification:** *Mixed — transient and permanent failures are
conflated.* The `stage="exception"` is a grab-bag. The retry is correct
for transient failures and wasteful for permanent ones. No distinction
is made.

**Wasted compute:** For permanent failures: `max_attempts` × (context
read + prompt build + LLM call attempt + rollback). Each attempt burns a
full prompt rendering cycle before hitting the same auth error.

**Correctness risk:** Low. The LLM is never successfully called, so no
writes are applied.

### F3: `llm_output_invalid` — LLM response parse failure

**Origin:** `nodes_se.py:250-271`

The LLM returned text that failed `json.loads()` or Pydantic validation
of `WriteProposal`.

**Can retry change the outcome?** Maybe. The LLM is stochastic. With
`temperature=0`, the same prompt can still produce different outputs
across calls (model-level non-determinism). The retry prompt includes
the parse error, which gives the LLM corrective signal.

But: the retry prompt includes the error message, not the raw malformed
response. The LLM sees "Parse error: ..." and the original work order,
but not its own failed output. It cannot diff its attempt against the
error. The corrective signal is weak.

**Classification:** *Partially justified.* The retry has a chance of
working because the LLM output is stochastic. But the corrective signal
is minimal — the LLM doesn't see what it did wrong, only that something
failed.

**Wasted compute:** Moderate. Each retry is a full LLM call. At
$0.01–$0.10 per call and 30-120 seconds of latency, `max_attempts`
retries on a model that consistently produces malformed output waste
both money and time.

**Correctness risk:** None. No writes are applied on parse failure.

### F4: `write_scope_violation` — Proposal writes to disallowed files

**Origin:** `nodes_tr.py:105-142` (three sub-cases: duplicate paths,
out-of-scope paths, path-escapes-repo)

The LLM proposed writing to files not in `allowed_files`, or proposed
duplicate paths, or proposed a path that resolves outside the repo root.

**Can retry change the outcome?** Maybe, for the same reason as F3 —
the LLM is stochastic and the error is fed back. But the failure brief
says things like `"Files outside allowed scope: ['bad.py']"` and the
`constraints_reminder` says `"All proposed file paths must be in the
work order's allowed_files list."` The LLM already had the
`allowed_files` list in its original prompt. If it ignored the list
once, feeding back "you ignored the list" may or may not help.

For path-escapes-repo: this is likely a persistent LLM confusion about
relative paths. Retry is unlikely to fix it.

**Classification:** *Partially justified, but fragile.* The corrective
signal is slightly better than F3 because the error is specific. But
the underlying cause (LLM ignoring the allowed_files constraint) is not
reliably fixed by restating the constraint.

**Wasted compute:** Moderate. Full LLM call per retry.

**Correctness risk:** None. TR rejects the proposal before any writes.

### F5: `stale_context` — Base-hash mismatch

**Origin:** `nodes_tr.py:147-164`

The LLM's `base_sha256` for a file doesn't match the file's current
content on disk.

**Can retry change the outcome?** No, in the normal case. After rollback,
the repo is restored to baseline. SE re-reads context files (which will
have baseline hashes). The LLM sees the same context and should produce
the correct hashes. So in theory, retry can fix this if the LLM made a
hash error on the first attempt.

But there's a subtlety: the prompt shows the LLM the exact `sha256` for
each context file (`nodes_se.py:112`). If the LLM copies the hash
correctly, it cannot fail. A `stale_context` failure means the LLM
either fabricated a hash (ignored the prompt's value) or there was a
TOCTOU race (file changed between SE's read and TR's check). The latter
is near-impossible under the sole-writer assumption.

So: retry can work if the LLM hallucinated a hash and the retry prompt
makes it more careful. But if the LLM consistently hallucinates hashes,
retrying changes nothing.

**Classification:** *Partially justified.* The LLM is the likely source
of error and is stochastic, but the root cause (ignoring a value shown
in the prompt) may persist across retries.

**Wasted compute:** Full LLM call per retry.

**Correctness risk:** None. TR rejects before any writes.

### F6: `write_failed` — Atomic file write failed

**Origin:** `nodes_tr.py:169-181`

The `_atomic_write` call raised an exception. This is an OS-level error:
permission denied, disk full, filesystem read-only, etc.

**Can retry change the outcome?** Almost never. The retry goes through
rollback → SE → LLM call → TR → same `_atomic_write` to the same path.
The OS-level condition (disk full, permissions) is unchanged. The retry
wastes a full LLM call for nothing.

Exception: if the error was a transient filesystem hiccup (e.g., NFS
stale handle), retry might work. This is exotic.

**Classification:** *Deterministic failure retried (nearly always).*

**Wasted compute:** Full LLM call per retry + the cost of re-running
all validation checks. High waste.

**Correctness risk:** Low directly, but there is a subtle danger. If the
write failure occurred partway through a multi-file proposal (file K
succeeded, file K+1 failed), the repo has partial writes. `_tr_fail`
returns without rollback — rollback happens later in finalize. But the
`_tr_fail` return records `touched_files` as all files in the proposal,
not just the ones that were actually written. This mislabeling is
cosmetic but could confuse post-mortem analysis.

### F7: `verify_failed` — Global verification command failed

**Origin:** `nodes_po.py:90-113`

A verify command (`bash scripts/verify.sh` or fallback) exited non-zero.

**Can retry change the outcome?** Yes. This is the *intended* retry
case. The LLM wrote code that doesn't pass tests. On retry, the LLM
sees the verify error output and can attempt a fix. The context files
are re-read after rollback (so the LLM sees the original baseline
again, not its broken attempt), and the failure brief tells it what
went wrong.

**Classification:** *Correctly retried.* This is the core use case for
the retry loop.

**Wasted compute:** Expected and justified.

**Correctness risk:** None. Rollback restores baseline before retry.

### F8: `acceptance_failed` — Acceptance command or postcondition failure

**Origin:** `nodes_po.py:121-199` (three sub-cases: postcondition
failure, shlex parse failure, command exit non-zero)

**Can retry change the outcome?** Depends on sub-case:

- **Postcondition failure (file doesn't exist after writes):** Yes, if
  the LLM forgot to create a file. The retry prompt tells it which file
  was missing. Same justification as F7.

- **shlex parse failure:** No. The acceptance command string comes from
  the work order JSON, which is immutable across attempts. If `shlex`
  can't parse it on attempt 1, it can't parse it on attempt 5. This is
  a planner bug (should have been caught by E007), not an executor error.
  But the planner validation might have been skipped or the work order
  might be hand-written.

- **Acceptance command exit non-zero:** Yes, same justification as F7.
  The LLM wrote code that doesn't pass the acceptance test.

**Classification:** *Mostly correct.* The shlex sub-case is a
deterministic failure retried, but it's a narrow edge case. The other
two sub-cases are correctly retried.

**Wasted compute:** For the shlex case: `max_attempts` × full pipeline.
For the others: expected and justified.

**Correctness risk:** None.

---

## 3. Deterministic Failures That Are Retried

| ID | Stage | Origin | Why retry is wrong | Waste per retry |
|----|-------|--------|--------------------|-----------------|
| F1 | `preflight` | `nodes_se.py:168-213` | Filesystem state is identical after rollback. The code literally says "PLANNER-CONTRACT BUG" and "Re-run the planner" then retries. | Artifact dir + failure_brief + rollback (no LLM) |
| F6 | `write_failed` | `nodes_tr.py:169-181` | OS-level error (disk full, perms) persists across retries. Burns a full LLM call on each retry before hitting the same OS error. | Full LLM call + prompt build + validation |
| F8a | `acceptance_failed` (shlex) | `nodes_po.py:150-173` | Command string is from the work order (immutable). `shlex.split` failure is deterministic. Burns a full LLM call + writes + verify before hitting the same parse error. | Full pipeline (LLM + writes + verify) |

### Total waste for `max_attempts=5`:

- **F1:** 5 × (mkdir + save_json + rollback). ~5 seconds of useless I/O.
  No LLM cost. Misleading `total_attempts: 5` in summary.

- **F6:** 5 × (LLM call at ~$0.01-0.10 + 30-120s latency + prompt build
  + TR validation). Up to $0.50 and 10 minutes of compute on a disk-full
  error that was visible on attempt 1.

- **F8a:** 5 × (LLM call + TR writes + PO verify + PO acceptance up to
  the broken command). The most wasteful case: every retry does real work
  (writes files, runs tests) before hitting a static parse error.

---

## 4. Transient Failures That Are NOT Retried (but should be)

### T1: LLM API transient errors are retried at the wrong layer

**Location:** `factory/llm.py:36-52`

The factory's LLM client has **zero** transport-level retry logic. Compare
with the planner's client (`planner/openai_client.py`) which has explicit
retry on HTTP 429/502/503/504 with backoff (`MAX_TRANSPORT_RETRIES=3`,
`TRANSPORT_RETRY_BASE_S=3.0`).

The factory relies entirely on the `openai` SDK's built-in retry behavior.
As of the `openai` Python SDK v1.x, it retries on 429, 500, 502, 503, 504
with exponential backoff (2 retries by default). So transient errors *are*
retried — but at the SDK layer, not the application layer, and with
different parameters than the planner.

If the SDK's 2 retries are exhausted, the exception propagates to
`nodes_se.py:239` and becomes `stage="exception"`. This then triggers a
full graph-level retry: rollback → re-read context → re-build prompt →
re-call LLM. The graph-level retry is extremely expensive compared to a
simple HTTP-level retry with backoff.

**What's missing:** An application-level retry with backoff between the SDK
and the graph-level retry. The gap is: SDK exhausts its 2 retries (fast,
cheap), then the next retry is a full graph cycle (slow, expensive) with no
additional backoff. A 429 rate-limit that persists for 60 seconds will fail
the SDK retries (which are fast), fail the graph retry (which re-calls
immediately), fail the second graph retry (same), and exhaust
`max_attempts`.

**Waste:** A rate-limit event that would resolve with a 60-second sleep
instead burns `max_attempts` graph cycles (each ~30-120 seconds of LLM
latency + prompt build + rollback), exhausts all attempts, and produces a
FAIL verdict.

**Correctness risk:** None (no writes applied), but the run fails when it
shouldn't.

### T2: Command timeouts are not distinguished from logic errors

**Location:** `factory/util.py:155-173`

When a verify or acceptance command times out, `run_command` returns
`CmdResult(exit_code=-1, ...)` with `[TIMEOUT]` appended to stderr. This
flows through PO as `verify_failed` or `acceptance_failed`. The retry
treats it identically to a logic error (the LLM wrote broken code).

**What's missing:** A timeout might be transient (system load spike, slow
I/O) or might indicate an infinite loop in LLM-generated code. The system
cannot tell. But the retry prompt tells the LLM "your code failed" and
asks it to fix the problem. If the timeout was transient, the LLM
re-writes perfectly good code unnecessarily. If the timeout was an
infinite loop, the retry produces a different implementation that might
also loop.

This isn't a "missing retry" exactly — it's a failure to distinguish
timeout-as-transient from timeout-as-bug. The result is that transient
timeouts waste LLM calls on code that was correct, and infinite-loop
timeouts may never converge because the LLM doesn't know it needs to
fix a loop (the error message says `[TIMEOUT]` not "your code has an
infinite loop").

**Waste:** Full LLM call when a simple re-run of the command might pass.

---

## 5. The Failure Brief as Corrective Signal

The retry mechanism's corrective power depends entirely on the failure
brief injected into the SE prompt. Here is an honest assessment of each:

| Stage | What the LLM sees | Can the LLM act on it? |
|-------|-------------------|----------------------|
| `preflight` | "PLANNER-CONTRACT BUG: precondition..." | No. The LLM didn't cause this and can't fix it. The brief is injected into the prompt for a problem the LLM has no control over. |
| `exception` | "LLM API call failed. Check OPENAI_API_KEY..." | No. This is an infrastructure error. The LLM can't fix its own API key. |
| `llm_output_invalid` | "Parse error: ... Raw response (first 500 chars)..." | Weakly. The LLM sees the error type but not its full failed output. |
| `write_scope_violation` | "Files outside allowed scope: [...]" | Weakly. The LLM already had the allowed_files list. Being told it violated the list is a restatement, not new information. |
| `stale_context` | "Hash mismatch for X: expected Y, actual Z" | Weakly. The LLM already had the correct hash in the prompt. |
| `write_failed` | "Failed to write X: [OSError]" | No. This is an OS error. |
| `verify_failed` | Truncated stderr/stdout of the failed command | Yes. This is actionable — the LLM sees test errors and can fix code. |
| `acceptance_failed` | Truncated stderr/stdout or postcondition message | Yes (for code errors). No (for shlex parse failures). |

Only `verify_failed` and `acceptance_failed` (code-error sub-case) provide
corrective signal that the LLM can meaningfully act on. The other stages
inject noise into the prompt — the LLM sees an error it didn't cause and
can't fix, which may confuse it into making unnecessary changes.

---

## 6. Summary

| # | Problem | Type | Severity |
|---|---------|------|----------|
| F1 | Preflight (precondition) failures retried | Deterministic failure retried | Operational — waste + misleading artifacts |
| F6 | OS-level write failures retried | Deterministic failure retried | Operational — wastes LLM calls on disk errors |
| F8a | shlex parse failure retried | Deterministic failure retried | Operational — wastes full pipeline per retry |
| T1 | No application-level LLM retry with backoff | Transient failure not handled at correct layer | Operational — rate limits exhaust all attempts |
| T2 | Timeouts conflated with logic errors | Transient failure misclassified | Operational — LLM rewrites correct code |
| — | Routing logic has no failure classification | Architectural | All of the above stem from this |

The root cause is that `_route_after_finalize` is a single `if attempts_left: retry`
check with no failure classification. Every failure is treated as "the LLM
should try again." This is correct for verify/acceptance failures (the
intended use case) and wrong for everything else.
