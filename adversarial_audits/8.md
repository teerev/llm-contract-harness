# Production Readiness Assessment

Blunt evaluation of whether this system can be run unattended, what
breaks first, what it assumes, and what is clearly demo-only.

Based on audits `1.md` through `7.md` and direct code inspection.

---

## 1. What Would Prevent Running This Unattended on Important Code

### 1a. Unsandboxed arbitrary command execution

The factory runs acceptance commands and `scripts/verify.sh` as the
operator's user with no sandbox, no network restriction, no filesystem
isolation, and no capability dropping (`4.md` V4). The commands are
authored by the planner LLM and executed verbatim. An adversarial or
confused LLM can `curl` your SSH keys to an external server, `rm -rf`
your home directory, or install persistent backdoors.

Running this unattended on a machine with access to production secrets,
SSH keys, cloud credentials, or internal networks is irresponsible.
The only safe way to run it unattended is inside a disposable container
with no network egress and no access to sensitive filesystems — which
is not part of the system and not documented as a requirement.

### 1b. No crash recovery

If the process is killed (OOM, SIGKILL, power loss) during file writes,
the repo is left in a state that never existed in any correct execution
(`2.md` W1, W2). There is no write-ahead log, no transaction marker,
and no automatic recovery on restart. The preflight `is_clean` check
blocks the next run, but the operator must manually determine the
baseline commit and run `git reset --hard && git clean -fdx`. On an
unattended system, this means the pipeline stops and stays stopped until
a human intervenes.

### 1c. No idempotency

Re-executing a completed work order wastes LLM calls, risks semantic
regression (the LLM may produce different code), and silently overwrites
the previous run's artifacts including the PASS summary (`1.md` issues
1-6). An unattended system that retries after a crash or reschedules a
work order will produce unpredictable results. There is no "skip if
already done" capability at any layer.

### 1d. Semantic verification is LLM-on-LLM

The planner LLM writes the tests. The factory LLM writes the code.
The factory checks the code against the tests. If the tests are weak
(existence checks, type checks), the LLM can satisfy them with stubs
(`6.md` S1-S4). There is no human-authored golden test, no property-based
testing, no coverage measurement, and no independent verification. On
important code, you need at least one layer of verification that was not
authored by the same process that wrote the code.

### 1e. No observability

There is no logging framework. The factory uses `print()` to stderr
for errors and stdout for verdicts (`run.py`). The planner's OpenAI
client has a `_log()` function that prints to stderr
(`openai_client.py:43`). There are no log levels, no structured logs,
no metrics, no alerts, no health checks, no heartbeats. An unattended
system would need at minimum a process supervisor, log aggregation, and
alerting on FAIL/ERROR verdicts — none of which exist or are documented.

---

## 2. What Would Break First Under Repeated Use

### 2a. Artifact directory collision

The `run_id` is `sha256(work_order + baseline_commit)[:16]`. Running
the same work order against the same commit (e.g., retrying after a
failure without committing) produces the same `run_id` and the same
artifact directory. The second run silently overwrites the first run's
`run_summary.json` and per-attempt artifacts (`1.md` issue 5). After
a few dozen runs, the artifact directory becomes an unreliable record
of what actually happened.

### 2b. Deterministic failures consuming all retries

Precondition failures (plan-level bugs) are retried `max_attempts`
times despite being deterministic (`3.md` F1). OS-level write failures
(disk full) waste LLM calls on each retry (`3.md` F6). Under repeated
use, these burn through API budget and produce misleading
`total_attempts: N` counts in summaries.

### 2c. LLM cost accumulation without feedback

There is no cost tracking, no token counting, no budget enforcement.
Every retry is a full LLM call. With `max_attempts=5` and a planner
that produces 6 work orders, a single bad plan can cost 30 LLM calls
(6 WOs × 5 attempts) before the operator notices. The system does not
report cumulative cost. The compile summary records usage stats from the
planner's OpenAI API response, but the factory records nothing about
LLM cost — no token counts, no model pricing, no cumulative spend.

### 2d. `verify.sh` fragility

The global verify script (`scripts/verify.sh`) is written by the SE LLM
in WO-01. Its quality depends entirely on the planner's notes and the
LLM's interpretation. If it's broken (wrong test runner, missing
dependencies, hardcoded paths), every subsequent work order fails verify.
The verify-exempt mechanism protects WO-01 itself, but WO-02 onward
runs the LLM-authored verify script. A broken verify.sh blocks the
entire pipeline.

---

## 3. Silent Assumptions About the Operator and Environment

### 3a. Sole-writer assumption

The factory assumes it is the only process modifying the target repo
during execution. There is no file locking, no advisory lock on the
git index, no flock on the repo directory. If another process (or
another factory instance) writes to the repo concurrently, base-hash
checks may pass on stale data, rollback may discard someone else's
work, and the preflight `is_clean` check may pass between another
process's modifications.

**Not documented.** INVARIANTS.md mentions the sole-writer assumption
in passing (L3 comment) but does not state it as a prerequisite.

### 3b. Python environment

The factory assumes `python` on `PATH` is the right Python. It assumes
`pip`, `pytest`, `compileall` are available. It assumes the correct
virtual environment is activated. None of this is checked at startup.
The verify fallback commands include `python -m pip --version` which
tests that pip exists but not that the right packages are installed.

If the operator has a system Python without pytest, the fallback verify
fails with "No module named pytest" and the factory records a
`verify_failed` error — which looks like the LLM wrote broken code when
in fact the environment is wrong.

### 3c. Git configuration

The factory assumes `git` is installed and on `PATH`. It assumes the repo
has at least one commit (otherwise `git rev-parse HEAD` fails). It does
not check the git version. Different git versions have different behaviors
for `git clean`, `git add`, and `git write-tree`. The batch script
(`run_work_orders.sh`) assumes `git config user.email` and `user.name` are
either set globally or can be set locally.

### 3d. Network access for LLM calls

The factory assumes unrestricted HTTPS access to `api.openai.com`. If the
operator is behind a corporate proxy, VPN, or firewall, LLM calls fail
with opaque connection errors classified as `stage="exception"`. There is
no proxy configuration, no environment variable support for `HTTPS_PROXY`,
and no diagnostic for network connectivity.

### 3e. Disk space

No check for available disk space before writing files, artifacts, or
git objects. The factory creates per-attempt directories with prompt
text, JSON artifacts, and stdout/stderr captures. A large work order
sequence with many retries can generate hundreds of artifact files.
Running out of disk mid-write produces `write_failed` errors that
are retried (wasting LLM calls) because the retry logic doesn't
distinguish disk-full from code bugs (`3.md` F6).

### 3f. The `--repo` is expendable

The factory operates in-situ on the target repo. It runs `git reset
--hard` and `git clean -fdx` on failure. If the operator points
`--repo` at a repo with uncommitted work, the preflight `is_clean`
check catches it — but only if the work is unstaged or untracked.
Staged-but-uncommitted changes are also caught (git status --porcelain
reports them). But the operator must understand that a successful run
leaves the repo in a modified state (the LLM's code is on disk but
not committed — the batch script handles committing, but direct
`python -m factory run` does not).

---

## 4. What Is Clearly Demo-Only

### 4a. The batch execution script (`utils/run_work_orders.sh`)

A shell script that wipes and re-inits the target repo, iterates
WO-*.json files, calls the factory, and `git commit --no-verify` after
each. Hardcoded git identity (`factory@aos.local`), stops on first
failure with no resume capability, no parallel execution, no progress
reporting, no cost tracking. The user has indicated this is being
deleted.

### 4b. The scoring tool (`utils/score_work_orders.py`)

Reads from four hardcoded directories (`./wo`, `./wo2`, `./wo3`, `./wo4`).
No CLI arguments. Produces a terminal dump followed by `---JSON---` and
a JSON blob. No integration with the factory or planner. A standalone
analysis tool that was useful during development and irrelevant for
production. Also being deleted.

### 4c. The spec.txt file

A toy spec for a "terminal-based word guessing game." This is the
development test case. Production use would require a mechanism for
operators to supply specs, which does not exist beyond the `--spec`
CLI flag.

### 4d. The absence of packaging

No `pyproject.toml`, `setup.py`, `setup.cfg`, `requirements.txt`,
`Dockerfile`, `Makefile`, CI configuration, or any other packaging or
deployment artifact. The system is a collection of Python modules run
via `python -m`. Dependencies (`pydantic`, `langgraph`, `openai`,
`httpx`) are mentioned in the README but not pinned, not locked, and
not declared in any machine-readable format. `pip install pydantic
langgraph openai httpx` is the documented installation procedure.

---

## Verdict: **Prototype**

Not demo. Not production-adjacent. Prototype.

### Why not "demo"

The codebase has genuine engineering substance. The deterministic
enforcement layer (path validation, scope checking, base-hash
verification, precondition/postcondition gates) is well-designed and
thoroughly tested (472 tests). The planner's compile-retry loop with
structured error codes is a real validation pipeline. The M-01 through
M-10 contract fixes address real adversarial audit findings. The M-14
through M-19 configuration extraction is proper infrastructure work.
This is not a hackathon project or a proof-of-concept sketch.

### Why not "production-adjacent"

Production-adjacent would require:

1. **Sandboxing for command execution.** The single most critical gap.
   Without containerization or at minimum `unshare`/`firejail`/seccomp,
   the factory is an arbitrary-code-execution-as-a-service for whatever
   the LLM decides to write. This is not a hardening polish item — it's
   a prerequisite for running on any machine you care about.

2. **Crash recovery.** A write-ahead log or transaction marker that
   enables automatic rollback on restart, instead of requiring manual
   `git reset --hard`. Without this, any OOM or power loss requires
   operator intervention.

3. **Idempotency.** The ability to re-run a work order without side
   effects if it's already complete. Without this, crash recovery
   requires knowing exactly where the pipeline stopped, which the
   system doesn't record.

4. **Logging and observability.** Structured logs, token/cost tracking,
   error classification in machine-readable output, health checks. The
   current `print()`-to-stderr approach is insufficient for unattended
   operation.

5. **Packaging.** Pinned dependencies, a container image, and
   documentation of host requirements. The current "pip install four
   packages and hope for the best" is not a deployment story.

6. **Failure classification in the retry loop.** The single
   `if attempts_left: retry` router is the most impactful code-level
   issue. Distinguishing deterministic failures (preflight, disk-full)
   from retryable failures (LLM output, test failures) would
   immediately reduce wasted compute and improve diagnostic clarity.

### Why "prototype" and not lower

The word "prototype" implies: the architecture is sound, the core
mechanisms work, the test suite provides confidence, and the system
can be used for its intended purpose under supervised conditions.
All of these are true.

The system successfully:
- Decomposes specs into validated work-order sequences
- Enforces file scope, path safety, and hash integrity mechanically
- Retries with corrective feedback when the LLM produces broken code
- Rolls back on failure (when the process doesn't get killed)
- Produces a comprehensive artifact trail for post-mortem analysis

What it lacks is everything that separates "works on my machine under
my supervision" from "can be left running overnight on a server."
That gap is the definition of prototype-to-production.
