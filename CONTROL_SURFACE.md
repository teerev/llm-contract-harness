# CONTROL_SURFACE.md — Forensic Control-Surface Audit

> Generated by direct code inspection of all production files in `factory/`,
> `planner/`, `utils/`, and top-level entry points.
> Every claim is traced to a specific file and line.
> "NOT PRESENT" means the item was looked for and not found.

---

## 1. EXECUTION ENTRY POINTS

### 1.1 `python -m factory run`

| Field | Value |
|---|---|
| File | `factory/__main__.py` → `factory/run.py::run_cli()` |
| Invocation | `python -m factory run --repo PATH --work-order PATH --out PATH --llm-model MODEL [--max-attempts N] [--llm-temperature F] [--timeout-seconds S]` |
| User-facing | Yes — primary factory entry point |
| Exit codes | `0` = PASS, `1` = FAIL or validation error or missing subcommand, `2` = unhandled exception, `130` = KeyboardInterrupt (SIGINT) |

The `factory/__main__.py::main()` function parses args, validates `--max-attempts >= 1`,
then defers to `factory/run.py::run_cli(args)`.

`run_cli()` performs preflight checks (git repo, clean tree, out-dir not inside repo),
builds a LangGraph state machine, invokes it, and writes `run_summary.json`.

### 1.2 `python -m planner compile`

| Field | Value |
|---|---|
| File | `planner/__main__.py` → `planner/cli.py::main()` → `planner/cli.py::_run_compile()` → `planner/compiler.py::compile_plan()` |
| Invocation | `python -m planner compile --spec PATH --outdir PATH [--template PATH] [--artifacts-dir PATH] [--repo PATH] [--overwrite] [--print-summary]` |
| User-facing | Yes — primary planner entry point |
| Exit codes | `0` = success, `1` = general error (missing file, no subcommand), `2` = validation errors in output, `3` = API/network error, `4` = JSON parse error in LLM output |

The `planner/__main__.py` module calls `sys.exit(main())`.
`cli.py::main()` dispatches to `_run_compile()` which calls `compiler.py::compile_plan()`.

### 1.3 `utils/run_work_orders.sh`

| Field | Value |
|---|---|
| File | `utils/run_work_orders.sh` |
| Invocation | `./utils/run_work_orders.sh --wo-dir DIR --target-repo DIR --artifacts-dir DIR [--model NAME] [--max-attempts N] [--no-init]` |
| User-facing | Yes — orchestration wrapper for sequential WO execution |
| Exit codes | `0` = all passed, `1` = arg error or first WO failure (breaks loop) |

This shell script:
1. Optionally wipes and `git init`s the target repo (unless `--no-init`).
2. Iterates sorted `WO-*.json` files.
3. Calls `python -m factory run` for each.
4. On success: `git add -A && git commit --no-verify`.
5. On failure: `break` immediately (no rollback — factory handles that).

### 1.4 `utils/score_work_orders.py`

| Field | Value |
|---|---|
| File | `utils/score_work_orders.py::main()` |
| Invocation | `python utils/score_work_orders.py` |
| User-facing | Yes — diagnostic / scoring tool |
| Exit codes | `0` always (no explicit non-zero exit) |

Reads work orders from hardcoded directories `["./wo", "./wo2", "./wo3", "./wo4"]`
(line 17). Prints a human-readable terminal summary followed by a `---JSON---`
delimiter and machine-readable JSON.

### 1.5 Internal function entry points

| Function | File | Role |
|---|---|---|
| `run_cli(args)` | `factory/run.py:20` | Core factory orchestrator (called by `__main__`) |
| `compile_plan(...)` | `planner/compiler.py:169` | Core planner orchestrator (called by CLI) |
| `build_graph()` | `factory/graph.py:159` | Constructs the LangGraph state machine |
| `se_node(state)` | `factory/nodes_se.py:150` | SE (Software Engineer) LLM node |
| `tr_node(state)` | `factory/nodes_tr.py:86` | TR (Transcriber/Writer) deterministic node |
| `po_node(state)` | `factory/nodes_po.py:65` | PO (Product Owner) verification node |
| `_finalize_node(state)` | `factory/graph.py:86` | Attempt finalization, rollback, retry gating |

---

## 2. EXISTING CLI FLAGS (EXPLICIT)

### 2.1 Factory CLI (`python -m factory run`)

Parsed in `factory/__main__.py:9-47`.

| Flag | Type | Default | Required | Consumed by | Scope |
|---|---|---|---|---|---|
| `--repo` | path | — | Yes | `run.py:22` → preflight, state init | Factory only |
| `--work-order` | path | — | Yes | `run.py:23` → `load_work_order()` | Factory only |
| `--out` | path | — | Yes | `run.py:24` → artifact output root | Factory only |
| `--max-attempts` | int | `2` | No | `run.py:89` → state, `graph.py:76` retry gating | Factory only |
| `--llm-model` | string | — | Yes | `run.py:91` → `nodes_se.py:231` LLM call | Factory only |
| `--llm-temperature` | float | `0` | No | `run.py:92` → `nodes_se.py:233` LLM call | Factory only |
| `--timeout-seconds` | int | `600` | No | `run.py:93` → `nodes_po.py:69`, `nodes_se.py:235` | Factory only |

**Enforcement notes:**
- `--max-attempts` is validated `>= 1` at `__main__.py:56-61`.
- `--llm-model` is required (no default). Passed directly to the OpenAI API; no validation of model name.
- `--timeout-seconds` is used for both LLM API timeout AND subprocess command timeout. These are conflated into a single value.

### 2.2 Planner CLI (`python -m planner compile`)

Parsed in `planner/cli.py:9-59`.

| Flag | Type | Default | Required | Consumed by | Scope |
|---|---|---|---|---|---|
| `--spec` | path | — | Yes | `compiler.py:196` → reads spec text | Planner only |
| `--outdir` | path | — | Yes | `compiler.py:171` → writes WO-*.json | Planner only |
| `--template` | path | `None` → resolved by `prompt_template.py:34-49` | No | `compiler.py:188` → prompt rendering | Planner only |
| `--artifacts-dir` | path | `None` → `./examples/artifacts` or `./artifacts` | No | `compiler.py:189-193` → compile artifacts | Planner only |
| `--repo` | path | `None` | No | `compiler.py:217-218` → builds file listing for chain validation | Planner only |
| `--overwrite` | bool | `False` | No | `compiler.py:367` → `io.py:30` overwrite check | Planner only |
| `--print-summary` | bool | `False` | No | `cli.py:126-129` → console summary | Planner only |

**Enforcement notes:**
- `--template` defaults to `planner/PLANNER_PROMPT.md` (resolved in `prompt_template.py:40-43`).
- `--artifacts-dir` has a two-level fallback: `./examples/artifacts` then `./artifacts` (`compiler.py:190-193`).
- Neither `--llm-model` nor `--llm-temperature` exist on the planner CLI. The planner uses hardcoded defaults (see Section 3).

### 2.3 Shell script flags (`utils/run_work_orders.sh`)

Parsed at lines 25-42.

| Flag | Type | Default | Required | Passed to |
|---|---|---|---|---|
| `--wo-dir` | path | — | Yes | Work order directory |
| `--target-repo` | path | — | Yes | `--repo` on factory call |
| `--artifacts-dir` | path | — | Yes | `--out` on factory call |
| `--model` | string | `"gpt-5.2"` | No | `--llm-model` on factory call |
| `--max-attempts` | int | `5` | No | `--max-attempts` on factory call |
| `--no-init` | bool | `false` | No | Skips repo wipe/init |

**CRITICAL ASYMMETRY:** The shell script defaults `--model` to `"gpt-5.2"` (line 20)
and `--max-attempts` to `5` (line 21), while the factory CLI defaults `--max-attempts`
to `2` (line 30 of `__main__.py`). The factory CLI has no default for `--llm-model`
(it is required). These defaults diverge.

**NOTE:** The shell script does NOT pass `--llm-temperature` or `--timeout-seconds`
to the factory, so those always take factory defaults (`0` and `600` respectively).

---

## 3. IMPLICIT CONFIGURATION (HIDDEN PARAMETERS)

This section catalogs every behavior-controlling value NOT exposed via CLI flags.

### 3.1 Planner LLM Configuration

| Parameter | Location | Value | Controls | Should be CLI-exposed? |
|---|---|---|---|---|
| `DEFAULT_MODEL` | `planner/openai_client.py:25` | `"gpt-5.2-codex"` | Which LLM model the planner uses | **Yes** — essential for model selection and cost control |
| `DEFAULT_REASONING_EFFORT` | `planner/openai_client.py:26` | `"medium"` | OpenAI reasoning effort parameter | **Yes** — directly affects output quality and cost |
| `DEFAULT_MAX_OUTPUT_TOKENS` | `planner/openai_client.py:27` | `64000` | Maximum output token budget for planner LLM | **Yes** — affects plan completeness |

These are imported by `compiler.py:18-23` and used in `compile_plan()` at lines 206-207
(compile hash computation) and indirectly via `OpenAIResponsesClient()` at line 228.
The `ModelConfig` dataclass (`openai_client.py:59-63`) wraps these but is never
overridden from CLI — `OpenAIResponsesClient()` is always called with `cfg=None`
(defaulting to `ModelConfig()` with the hardcoded values).

### 3.2 Planner Transport & Polling

| Parameter | Location | Value | Controls | Should be CLI-exposed? |
|---|---|---|---|---|
| `CONNECT_TIMEOUT` | `openai_client.py:32` | `30.0` s | HTTP connect timeout | No — infrastructure detail |
| `READ_TIMEOUT` | `openai_client.py:33` | `60.0` s | HTTP read timeout | No — infrastructure detail |
| `WRITE_TIMEOUT` | `openai_client.py:34` | `30.0` s | HTTP write timeout | No — infrastructure detail |
| `POOL_TIMEOUT` | `openai_client.py:35` | `30.0` s | Connection pool timeout | No — infrastructure detail |
| `MAX_TRANSPORT_RETRIES` | `openai_client.py:37` | `3` | HTTP-level retry count (429, 502, 503, 504) | No — but should be tunable in cloud config |
| `TRANSPORT_RETRY_BASE_S` | `openai_client.py:38` | `3.0` s | Base delay for exponential backoff | No |
| `POLL_INTERVAL_S` | `openai_client.py:40` | `5.0` s | Seconds between status polls | No — but may need tuning for rate limits |
| `POLL_DEADLINE_S` | `openai_client.py:41` | `2400.0` s (40 min) | Maximum wait for LLM response | **Yes (v2)** — long-running plans may need more |
| `MAX_INCOMPLETE_RETRIES` | `openai_client.py:43` | `1` | Retry count for incomplete (max_output_tokens) responses | No |

**Backoff strategy (planner):** Linear backoff with `delay = TRANSPORT_RETRY_BASE_S * attempt`
(`openai_client.py:230,254`). NOT exponential. The `Retry-After` header is respected
and the larger of header vs computed delay is used (`openai_client.py:230`).

**Incomplete-response handling:** On `max_output_tokens` incomplete, retries once with
`min(budget * 2, 65000)` (`openai_client.py:98-101`). The `65000` cap is hardcoded.

### 3.3 Factory LLM Configuration

| Parameter | Location | Value | Controls | Should be CLI-exposed? |
|---|---|---|---|---|
| LLM timeout default | `factory/llm.py:9,32` | `120` s | Per-request HTTP timeout to OpenAI | **Yes (v2)** — currently overridden by `--timeout-seconds` from CLI |
| LLM temperature default | `factory/llm.py:32` | `0` | Temperature parameter | Already exposed via `--llm-temperature` |

The factory LLM client (`factory/llm.py`) uses the standard `openai` SDK (Chat
Completions API), NOT the Responses API used by the planner. The `timeout` parameter
in `_get_client()` (line 9) defaults to `120` but is overridden by
`state["timeout_seconds"]` passed from the CLI (see `nodes_se.py:235`).

### 3.4 Planner Compile Loop

| Parameter | Location | Value | Controls | Should be CLI-exposed? |
|---|---|---|---|---|
| `MAX_COMPILE_ATTEMPTS` | `compiler.py:37` | `3` | 1 initial + up to 2 retries on validation failure | **Yes** — controls planner self-correction budget |
| `_SKIP_DIRS` | `compiler.py:40-41` | `{".git", "__pycache__", ".pytest_cache", "node_modules", ".mypy_cache", ".tox", ".venv", "venv", ".eggs"}` | Directories excluded from repo file listing | No — internal safety list |

### 3.5 Factory Size Limits & Thresholds

| Parameter | Location | Value | Controls | Should be CLI-exposed? |
|---|---|---|---|---|
| `MAX_FILE_WRITE_BYTES` | `factory/schemas.py:130` | `200 * 1024` (200 KB) | Max size of a single file write | No — safety invariant |
| `MAX_TOTAL_WRITE_BYTES` | `factory/schemas.py:131` | `500 * 1024` (500 KB) | Max total size of all writes in a proposal | No — safety invariant |
| `MAX_JSON_PAYLOAD_BYTES` (factory) | `factory/llm.py:50` | `10 * 1024 * 1024` (10 MB) | Max size of LLM response JSON before parsing | No — defense-in-depth (M-10) |
| `MAX_JSON_PAYLOAD_BYTES` (planner) | `planner/compiler.py:70` | `10 * 1024 * 1024` (10 MB) | Max size of LLM response JSON before parsing | No — defense-in-depth (M-10) |
| `MAX_CONTEXT_BYTES` | `factory/nodes_se.py:20` | `200 * 1024` (200 KB) | Total budget for context file reading in SE prompt | No — controls prompt size |
| `MAX_EXCERPT_CHARS` | `factory/util.py:55` | `2000` | Truncation limit for error excerpts | No — display concern |
| Context files limit | `factory/schemas.py:106-108` | `10` | Max entries in `context_files` per work order | No — schema constraint |

### 3.6 Factory Git Configuration

| Parameter | Location | Value | Controls | Should be CLI-exposed? |
|---|---|---|---|---|
| `GIT_TIMEOUT_SECONDS` | `factory/workspace.py:11` | `30` | Timeout for all git subprocess calls | No — internal safety bound |

### 3.7 Factory Run ID

| Parameter | Location | Value | Controls | Should be CLI-exposed? |
|---|---|---|---|---|
| Run ID truncation length | `factory/util.py:48` | `16` hex chars | Length of `sha256(work_order + baseline_commit)` used as run directory name | No — deterministic |
| Compile hash truncation | `planner/compiler.py:63` | `16` hex chars | Length of `sha256(spec + template + model + reasoning)` used for artifact directory | No — deterministic |

### 3.8 OpenAI API Endpoint

| Parameter | Location | Value | Controls | Should be CLI-exposed? |
|---|---|---|---|---|
| `OPENAI_API_BASE` | `planner/openai_client.py:19` | `"https://api.openai.com/v1"` | API base URL (planner uses Responses API) | **Yes (v2)** — needed for proxies, Azure, etc. |
| API base (factory) | `factory/llm.py:28` | Implicit (openai SDK default) | API base URL (factory uses Chat Completions) | **Yes (v2)** — same reason |

### 3.9 Environment Variables

| Variable | Location(s) | Required | Controls |
|---|---|---|---|
| `OPENAI_API_KEY` | `factory/llm.py:15`, `planner/openai_client.py:75` | Yes (both) | Authentication for OpenAI API |

This is the **only** environment variable used in production code. Both the planner
and factory check for it and raise `RuntimeError` if absent.

There is NO support for `OPENAI_API_BASE`, `OPENAI_ORG_ID`, or any other environment
variable override.

### 3.10 Implicit File Path Conventions

| Convention | Location | Value | Controls |
|---|---|---|---|
| Verify script path | `factory/nodes_po.py:27` | `scripts/verify.sh` | Global verification (if file exists) |
| Verify fallback commands | `factory/nodes_po.py:33-36` | `["python -m compileall -q .", "python -m pip --version", "python -m pytest -q"]` | Fallback if `scripts/verify.sh` absent |
| Verify-exempt lightweight check | `factory/nodes_po.py:83` | `["python -m compileall -q ."]` | Used when `verify_exempt=True` |
| WO file pattern | `utils/run_work_orders.sh:60` | `WO-*.json` | Glob for work order discovery |
| WO ID pattern | `planner/validation.py:16` | `^WO-\d{2}$` | Regex enforcing WO ID format |
| Verify command string | `planner/validation.py:14` | `"bash scripts/verify.sh"` | Expected verify command for validation |
| Verify script path (validation) | `planner/validation.py:15` | `"scripts/verify.sh"` | Path used in chain validator |
| Prompt template required placeholder | `planner/prompt_template.py:8` | `"{{PRODUCT_SPEC}}"` | Must appear in template |
| Prompt template optional placeholders | `planner/prompt_template.py:9` | `("{{DOCTRINE}}", "{{REPO_HINTS}}")` | Replaced with empty string if present |
| Factory prompt template path | `factory/nodes_se.py:65-67` | `factory/FACTORY_PROMPT.md` (same directory as module) | SE prompt template |
| Planner prompt default path | `planner/prompt_template.py:41-42` | `planner/PLANNER_PROMPT.md` (same directory as module) | Planner prompt template |

### 3.11 Shell Script Implicit Behaviors

| Behavior | Location | Detail |
|---|---|---|
| Git identity for init | `run_work_orders.sh:85-86` | `user.email="factory@aos.local"`, `user.name="AOS Factory"` |
| Seed files on init | `run_work_orders.sh:88-99` | Creates `README.md` and `.gitignore` with specific content |
| Commit flags | `run_work_orders.sh:128` | `git commit --no-verify` — skips pre-commit hooks |
| Stop on first failure | `run_work_orders.sh:134` | `break` on first non-zero factory exit |

### 3.12 Score Script Configuration

| Parameter | Location | Value | Controls |
|---|---|---|---|
| `WO_DIRS` | `utils/score_work_orders.py:17` | `["./wo", "./wo2", "./wo3", "./wo4"]` | Hardcoded directories to scan for WO files |

This is not configurable via CLI or environment. The script has no argument parsing.

---

## 4. LLM CONTROL SURFACES (PLANNER VS FACTORY)

### 4.1 Planner-Side LLM

| Aspect | Detail |
|---|---|
| **API** | OpenAI **Responses API** (POST `/v1/responses` with `background: true`, then poll GET) |
| **Client** | `planner/openai_client.py::OpenAIResponsesClient` — custom `httpx`-based client |
| **Model selection** | Hardcoded: `DEFAULT_MODEL = "gpt-5.2-codex"` (`openai_client.py:25`). NOT CLI-configurable. |
| **Temperature** | NOT PRESENT — the Responses API call has no temperature parameter. Only `reasoning.effort` is used. |
| **Reasoning effort** | `DEFAULT_REASONING_EFFORT = "medium"` (`openai_client.py:26`) |
| **Max output tokens** | `DEFAULT_MAX_OUTPUT_TOKENS = 64000` (`openai_client.py:27`), doubled to `65000` cap on incomplete retry |
| **Prompt source** | `planner/PLANNER_PROMPT.md` template, rendered by `prompt_template.py::render_prompt()` with spec text substitution |
| **Retry logic** | Compile loop: up to `MAX_COMPILE_ATTEMPTS = 3` (`compiler.py:37`). On validation failure, builds revision prompt with errors and re-calls LLM. On `max_output_tokens` incomplete: retries once with larger budget. |
| **Transport retries** | `MAX_TRANSPORT_RETRIES = 3` with linear backoff (`TRANSPORT_RETRY_BASE_S * attempt`). Retries on 429, 502, 503, 504, and `httpx` transport errors. |
| **Failure classification** | JSON parse error → retry with revision prompt. Validation errors → retry with revision prompt. API error → `RuntimeError` → exit 3. `incomplete` response → retry once then `RuntimeError`. |
| **Determinism assumptions** | Compile hash includes model + reasoning effort, so changing either changes the artifact directory. No temperature control → relies on model-level determinism. |
| **Malformed output** | If JSON is unparseable after all attempts → exit 4. If structurally invalid → errors stored, exit 2. Raw LLM responses are always persisted as artifacts. |

### 4.2 Factory-Side LLM

| Aspect | Detail |
|---|---|
| **API** | OpenAI **Chat Completions API** (standard `openai` SDK) |
| **Client** | `factory/llm.py::complete()` — thin wrapper around `openai.OpenAI().chat.completions.create()` |
| **Model selection** | CLI-configurable via `--llm-model` (required, no default). Passed through `state["llm_model"]`. |
| **Temperature** | CLI-configurable via `--llm-temperature` (default `0`). Passed through `state["llm_temperature"]`. |
| **Reasoning effort** | NOT PRESENT — Chat Completions API does not support this parameter. |
| **Max output tokens** | NOT PRESENT — not passed to Chat Completions API. Uses model default. |
| **Prompt source** | `factory/FACTORY_PROMPT.md` template, rendered by `nodes_se.py::_build_prompt()` with work-order fields, context files, and failure brief |
| **Retry logic** | Graph-level: up to `--max-attempts` (default 2) full SE→TR→PO cycles. On failure in any node, `_finalize_node` records the attempt, rolls back, and routes back to SE with a `failure_brief`. NO retry within the SE node for LLM errors — any exception becomes a `FailureBrief` and consumes one attempt. |
| **Transport retries** | NONE — the `openai` SDK's default retry behavior applies. No explicit retry wrapper in `factory/llm.py`. |
| **Failure classification** | LLM API exception → `FailureBrief(stage="exception")`. JSON parse error → `FailureBrief(stage="llm_output_invalid")`. Scope violation → `FailureBrief(stage="write_scope_violation")`. Stale hash → `FailureBrief(stage="stale_context")`. Write error → `FailureBrief(stage="write_failed")`. Verify fail → `FailureBrief(stage="verify_failed")`. Acceptance fail → `FailureBrief(stage="acceptance_failed")`. Precondition fail → `FailureBrief(stage="preflight")`. |
| **Determinism assumptions** | Temperature defaults to `0`. Single user message (no system prompt). Run ID is deterministic but based on work order + baseline commit, not on LLM output. |
| **Malformed output** | Attempts to strip markdown fences (`llm.py:60-65`). Falls back to `json.loads`. On failure, raw response is saved as artifact. The failed attempt is recorded and retry occurs if attempts remain. |

### 4.3 Asymmetry Summary

| Dimension | Planner | Factory |
|---|---|---|
| OpenAI API | Responses API (background polling) | Chat Completions API (synchronous) |
| HTTP client | Custom `httpx`-based | Standard `openai` SDK |
| Model | Hardcoded `gpt-5.2-codex` | CLI-provided (required) |
| Temperature | Not supported (reasoning API) | CLI: `--llm-temperature` (default `0`) |
| Reasoning effort | `medium` (hardcoded) | Not supported (Chat Completions) |
| Transport retries | Explicit 3-retry with backoff | Implicit (SDK default) |
| Semantic retries | Up to 3 compile attempts with validation feedback | Up to `max_attempts` full graph cycles with failure brief |
| Timeout control | `POLL_DEADLINE_S = 2400s` (hardcoded) | `--timeout-seconds` (default 600s), shared with subprocess timeout |

**Why asymmetric:** The planner uses the Responses API because plan generation requires
high reasoning effort and long-running requests (15-30 min). The factory uses Chat
Completions because SE proposals are simpler and faster. The planner's transport
complexity (background submission + polling) is a reliability pattern for long-lived
HTTP connections.

---

## 5. ARTIFACTS & OUTPUTS

### 5.1 Factory Artifacts

All artifact filenames are defined as constants in `factory/util.py:217-228`.

#### Per-Run Artifacts (in `{out_dir}/{run_id}/`)

| Artifact | File | Written when | Overwritable | Relied upon |
|---|---|---|---|---|
| Work order copy | `work_order.json` | Always, before graph invocation (`run.py:69`) | Yes (`save_json` uses atomic replace) | Post-mortem only |
| Run summary | `run_summary.json` | Always, after graph completes or on emergency (`run.py:203-204`, `run.py:165-166`) | Yes — emergency may overwrite | Terminal artifact — external consumers read this |

#### Per-Attempt Artifacts (in `{out_dir}/{run_id}/attempt_{N}/`)

| Artifact | Constant | Written when | Writer |
|---|---|---|---|
| SE prompt | `se_prompt.txt` | SE node, before LLM call (`nodes_se.py:226-227`) | `nodes_se.py` |
| Proposed writes | `proposed_writes.json` | SE node, on successful parse (`nodes_se.py:272-274`) | `nodes_se.py` |
| Raw LLM response | `raw_llm_response.json` | SE node, on parse failure only (`nodes_se.py:263-266`) | `nodes_se.py` |
| Write result | `write_result.json` | TR node, on success or failure (`nodes_tr.py:69-73`, `184-186`) | `nodes_tr.py` |
| Verify stdout | `verify_{idx}_stdout.txt` | PO node, per verify command (`nodes_po.py:94`) | `util.py::run_command()` |
| Verify stderr | `verify_{idx}_stderr.txt` | PO node, per verify command (`nodes_po.py:95`) | `util.py::run_command()` |
| Verify result | `verify_result.json` | PO node, after all verify commands or first failure (`nodes_po.py:107,114`) | `nodes_po.py` |
| Acceptance stdout | `acceptance_{idx}_stdout.txt` | PO node, per acceptance command (`nodes_po.py:178`) | `util.py::run_command()` |
| Acceptance stderr | `acceptance_{idx}_stderr.txt` | PO node, per acceptance command (`nodes_po.py:179`) | `util.py::run_command()` |
| Acceptance result | `acceptance_result.json` | PO node, after all acceptance commands or first failure (`nodes_po.py:193,201`) | `nodes_po.py` |
| Failure brief | `failure_brief.json` | SE node (write-ahead on LLM failure), finalize node (on any failure) (`nodes_se.py:244,268`, `graph.py:110`) | `nodes_se.py`, `graph.py` |

**Note:** `failure_brief.json` may be written twice per attempt — once write-ahead
by SE (for crash resilience) and once by finalize (canonical). The finalize write
overwrites the SE write (`graph.py:109` comment).

### 5.2 Planner Artifacts

#### Compile Artifacts (in `{artifacts_dir}/{compile_hash}/compile/`)

| Artifact | Written when | Writer |
|---|---|---|
| `prompt_rendered.txt` | Always, after prompt render (`compiler.py:222-224`) | `compiler.py` |
| `llm_raw_response_attempt_{N}.txt` | Per compile attempt, after LLM response (`compiler.py:240-243`) | `compiler.py` |
| `manifest_raw_attempt_{N}.json` | Per compile attempt, after successful JSON parse (`compiler.py:272-275`) | `compiler.py` |
| `validation_errors_attempt_{N}.json` | Per compile attempt, after validation (`compiler.py:258-261`, `298-301`) | `compiler.py` |
| `validation_errors.json` | On final failure only (`compiler.py:325`) | `compiler.py` |
| `manifest_normalized.json` | On success only (`compiler.py:363-365`) | `compiler.py` |
| `compile_summary.json` | Always, at end of `compile_plan()` (`compiler.py:404-406`) | `compiler.py::_write_summary()` |
| `raw_response_{label}.json` | On incomplete/failed/no-text LLM responses (`openai_client.py:299-311`) | `OpenAIResponsesClient._dump_response()` |

#### Output Artifacts (in `{outdir}/`)

| Artifact | Written when | Writer |
|---|---|---|
| `WO-{NN}.json` (per work order) | On success only (`compiler.py:368`) | `io.py::write_work_orders()` |
| `WORK_ORDERS_MANIFEST.json` | On success only, written LAST (`io.py:79`) | `io.py::write_work_orders()` |
| `validation_errors.json` | On final failure (`compiler.py:329`) | `compiler.py` |

### 5.3 Shell Script Artifacts

The shell script (`run_work_orders.sh`) creates:
- Git repo init (if `--no-init` not set): `README.md`, `.gitignore` in target repo
- Git commits after each successful WO: `git commit --no-verify`

No other artifacts are written by the shell script itself.

---

## 6. FAILURE MODES & EXIT SEMANTICS

### 6.1 Factory Failure Modes

| Trigger | Classification | Stage (FailureBrief) | Exit Code | Artifacts Written | Retry? |
|---|---|---|---|---|---|
| Work order JSON invalid | Contract error | N/A (exits before graph) | `1` (`run.py:33`) | None | No |
| Not a git repo | Preflight error | N/A | `1` (`run.py:39`) | None | No |
| Dirty working tree | Preflight error | N/A | `1` (`run.py:43`) | None | No |
| Out dir inside repo | Preflight error | N/A | `1` (`run.py:51`) | None | No |
| No subcommand given | Usage error | N/A | `1` (`__main__.py:53`) | None | No |
| `--max-attempts < 1` | Usage error | N/A | `1` (`__main__.py:58`) | None | No |
| Precondition file_exists false | Planner contract bug | `preflight` | Graph continues → `1` if final verdict FAIL | `failure_brief.json` | Yes (but pointless — same precondition will fail again) |
| Precondition file_absent false | Planner contract bug | `preflight` | Same | `failure_brief.json` | Yes (same) |
| LLM API call exception | Execution error | `exception` | Graph continues | `failure_brief.json` | Yes |
| LLM output unparseable | Execution error | `llm_output_invalid` | Graph continues | `raw_llm_response.json`, `failure_brief.json` | Yes |
| Proposal writes out of scope | Execution error | `write_scope_violation` | Graph continues | `write_result.json` | Yes |
| Duplicate paths in proposal | Execution error | `write_scope_violation` | Graph continues | `write_result.json` | Yes |
| Path escapes repo root | Execution error | `write_scope_violation` | Graph continues | `write_result.json` | Yes |
| Base hash mismatch | Execution error | `stale_context` | Graph continues | `write_result.json` | Yes |
| Atomic write fails | Execution error | `write_failed` | Graph continues | `write_result.json` | Yes |
| Verify command fails | Execution error | `verify_failed` | Graph continues | `verify_result.json` | Yes |
| Postcondition false after writes | Execution error | `acceptance_failed` | Graph continues | Empty `acceptance_result.json` | Yes |
| Acceptance command parse failure | Execution error | `acceptance_failed` | Graph continues | `acceptance_result.json` | Yes |
| Acceptance command fails | Execution error | `acceptance_failed` | Graph continues | `acceptance_result.json` | Yes |
| Command timeout | Execution error | (in CmdResult: `exit_code=-1`) | Graph continues | stdout/stderr files | Yes |
| Command OSError | Execution error | (in CmdResult: `exit_code=-1`) | Graph continues | stderr file | Yes |
| Unhandled exception in graph | Catastrophic | N/A | `2` (`run.py:183`) | Emergency `run_summary.json` with traceback | No |
| KeyboardInterrupt | User abort | N/A | `130` (`run.py:179`) | Emergency `run_summary.json` | No |
| All attempts exhausted, no PASS | Exhaustion | N/A | `1` (`run.py:210`) | `run_summary.json` with all attempt records | No |

**Rollback behavior:**
- On FAIL verdict: `_finalize_node` calls `rollback()` → `git reset --hard` + `git clean -fdx` (`graph.py:130`, `workspace.py:89-107`).
- On unhandled exception: `run_cli()` attempts best-effort rollback (`run.py:124-133`). Records `rollback_failed` and `remediation` command in emergency summary.
- On PASS: No rollback. Tree hash is computed from touched files only.

### 6.2 Planner Failure Modes

| Trigger | Classification | Exit Code | Artifacts Written |
|---|---|---|---|
| Spec file not found | Input error | `1` (`cli.py:91`) | None |
| Output dir exists without `--overwrite` | Safety error | `1` (`cli.py:93`) | None |
| API error / missing key | API error | `3` (`cli.py:101`) | Whatever was written before failure |
| Transport/network error | API error | `3` (`cli.py:106`) | Whatever was written before failure |
| JSON parse error (all attempts) | LLM output error | `4` (`cli.py:121`) | `llm_raw_response_attempt_*.txt`, `validation_errors_attempt_*.json`, `compile_summary.json` |
| Validation errors (all attempts) | Plan quality error | `2` (`cli.py:121`) | Same + `validation_errors.json` in both artifacts dir and outdir |
| No subcommand | Usage error | `1` (`cli.py:67`) | None |

### 6.3 Shell Script Failure Modes

| Trigger | Exit behavior | Recovery |
|---|---|---|
| Missing required args | `exit 1` (line 46) | Manual |
| WO dir doesn't exist | `exit 1` (line 51) | Manual |
| No WO-*.json files found | `exit 1` (line 65) | Manual |
| Factory returns non-zero | `break` (line 134), prints `FAILED` | Manual; repo state was rolled back by factory |
| Unknown flag | `exit 1` (line 34) | Manual |

---

## 7. CLI EXPOSURE RECOMMENDATIONS (ACTIONABLE)

### 7.1 Minimal Viable CLI Surface (v1)

These flags map directly to existing internal parameters with zero design changes.

#### `llmc plan` (planner)

| Flag | Maps to | Default | Risk |
|---|---|---|---|
| `--spec` | `planner/cli.py` `--spec` | — (required) | Low |
| `--outdir` | `planner/cli.py` `--outdir` | — (required) | Low |
| `--template` | `planner/cli.py` `--template` | `planner/PLANNER_PROMPT.md` | Low |
| `--artifacts-dir` | `planner/cli.py` `--artifacts-dir` | `./artifacts` | Low |
| `--repo` | `planner/cli.py` `--repo` | `None` | Low |
| `--overwrite` | `planner/cli.py` `--overwrite` | `False` | Low |
| `--model` | `DEFAULT_MODEL` in `openai_client.py:25` | `"gpt-5.2-codex"` | Low — essential |
| `--reasoning-effort` | `DEFAULT_REASONING_EFFORT` in `openai_client.py:26` | `"medium"` | Low |
| `--max-output-tokens` | `DEFAULT_MAX_OUTPUT_TOKENS` in `openai_client.py:27` | `64000` | Low |
| `--max-compile-attempts` | `MAX_COMPILE_ATTEMPTS` in `compiler.py:37` | `3` | Low |
| `--print-summary` | `planner/cli.py` `--print-summary` | `False` | Low |

#### `llmc run` (factory)

| Flag | Maps to | Default | Risk |
|---|---|---|---|
| `--repo` | `factory/__main__.py` `--repo` | — (required) | Low |
| `--work-order` | `factory/__main__.py` `--work-order` | — (required) | Low |
| `--out` | `factory/__main__.py` `--out` | — (required) | Low |
| `--llm-model` | `factory/__main__.py` `--llm-model` | — (required) | Low |
| `--llm-temperature` | `factory/__main__.py` `--llm-temperature` | `0` | Low |
| `--max-attempts` | `factory/__main__.py` `--max-attempts` | `2` | Low |
| `--timeout-seconds` | `factory/__main__.py` `--timeout-seconds` | `600` | Low |

#### `llmc batch` (shell script replacement)

| Flag | Maps to | Default | Risk |
|---|---|---|---|
| `--wo-dir` | `run_work_orders.sh` `--wo-dir` | — (required) | Low |
| `--target-repo` | `run_work_orders.sh` `--target-repo` | — (required) | Low |
| `--artifacts-dir` | `run_work_orders.sh` `--artifacts-dir` | — (required) | Low |
| `--model` | `run_work_orders.sh` `--model` | `"gpt-5.2"` | Low |
| `--max-attempts` | `run_work_orders.sh` `--max-attempts` | `5` | Low |
| `--no-init` | `run_work_orders.sh` `--no-init` | `false` | Low |

#### `llmc score` (scoring tool)

| Flag | Maps to | Default | Risk |
|---|---|---|---|
| `--wo-dirs` | `WO_DIRS` in `score_work_orders.py:17` | `["./wo"]` | Low |

### 7.2 Power-User CLI Surface (v2)

These flags expose tuning knobs that are currently hardcoded. Higher risk.

#### Planner transport tuning (`llmc plan`)

| Flag | Maps to | Default preserved? | Risk |
|---|---|---|---|
| `--poll-deadline` | `POLL_DEADLINE_S` (`openai_client.py:41`) | Yes (2400s) | Medium — too low causes false timeouts |
| `--api-base` | `OPENAI_API_BASE` (`openai_client.py:19`) | Yes | Medium — breaks if misconfigured |

#### Factory tuning (`llmc run`)

| Flag | Maps to | Default preserved? | Risk |
|---|---|---|---|
| `--llm-timeout` | LLM-specific timeout (currently conflated with `--timeout-seconds`) | Needs split | Medium — currently one value serves two purposes |
| `--cmd-timeout` | Subprocess-specific timeout | Needs split | Medium — same |
| `--api-base` | OpenAI base URL for factory LLM | Yes (SDK default) | Medium |

#### Batch tuning (`llmc batch`)

| Flag | Maps to | Default preserved? | Risk |
|---|---|---|---|
| `--continue-on-failure` | Inverse of current `break` behavior | No (currently stops) | Medium — partial plans are dangerous |
| `--llm-temperature` | Pass-through to factory | Yes (0) | Low |
| `--timeout-seconds` | Pass-through to factory | Yes (600) | Low |

---

## 8. NON-GOALS / THINGS THAT SHOULD NOT BE CLI FLAGS

The following are explicitly **not recommended** for CLI exposure.

### 8.1 Safety Invariants (MUST remain internal)

| Parameter | Reason |
|---|---|
| `MAX_FILE_WRITE_BYTES` (200 KB) | Safety bound. Exposing allows unbounded writes to target repo. |
| `MAX_TOTAL_WRITE_BYTES` (500 KB) | Same. |
| `MAX_JSON_PAYLOAD_BYTES` (10 MB) | Defense-in-depth against pathological LLM output. |
| `MAX_CONTEXT_BYTES` (200 KB) | Controls prompt size. Increasing risks token overflow. |
| `MAX_EXCERPT_CHARS` (2000) | Display concern only. |
| `context_files` limit (10) | Schema constraint. Changing breaks work-order format. |
| `GIT_TIMEOUT_SECONDS` (30) | Internal safety bound for git operations. |

### 8.2 Determinism-Sensitive Parameters

| Parameter | Reason |
|---|---|
| Run ID computation (`sha256[:16]`) | Changing breaks artifact directory reproducibility. |
| Compile hash computation | Same — also breaks artifact deduplication. |
| Canonical JSON serialization (sort_keys, separators) | Changing breaks hash reproducibility. |
| `rollback` strategy (`git reset --hard` + `git clean -fdx`) | Changing risks leaving repos dirty. The `-fdx` (not `-fd`) is deliberate (see `workspace.py:90-95`). |

### 8.3 Footgun Parameters

| Parameter | Reason |
|---|---|
| `_SKIP_DIRS` (planner repo scan) | Exposing invites scanning `.git/` or `node_modules/`. |
| `SHELL_OPERATOR_TOKENS` (validation) | Exposing weakens security validation. |
| `ALLOWED_STAGES` (FailureBrief) | Internal taxonomy. Changing breaks downstream tooling. |
| `verify_exempt` computation | Determined by `verify_contract`. Manual override would break safety. |
| Atomic write behavior (fsync + os.replace) | Crash safety invariant. |

### 8.4 Implicit Behaviors That Should Remain Implicit

| Behavior | Reason |
|---|---|
| Markdown fence stripping in JSON parse | LLM output normalization. Both planner and factory strip fences. Making configurable adds complexity for no benefit. |
| `sha256(b"")` for missing files | Convention used throughout. Changing breaks base-hash protocol. |
| `posixpath.normpath` for path normalization | Cross-platform safety. |
| Git identity in `run_work_orders.sh` | Only used for seed commits. Not relevant to production. |
| `--no-verify` on `git commit` in shell script | Prevents pre-commit hook interference with generated code. |

---

## Appendix A: File Index

All production files inspected in this audit:

```
factory/__init__.py
factory/__main__.py          — CLI argument parsing, entry point
factory/FACTORY_PROMPT.md    — SE prompt template
factory/graph.py             — LangGraph state machine definition
factory/llm.py               — LLM client (Chat Completions API)
factory/nodes_po.py          — PO node (verify + acceptance)
factory/nodes_se.py          — SE node (prompt + LLM + parse)
factory/nodes_tr.py          — TR node (scope check + atomic writes)
factory/run.py               — CLI orchestration, artifact IO
factory/schemas.py           — Pydantic models (WorkOrder, WriteProposal, etc.)
factory/util.py              — Hashing, truncation, JSON IO, command runner
factory/workspace.py         — Git operations (clean, rollback, tree hash)

planner/__init__.py
planner/__main__.py          — Entry point
planner/cli.py               — CLI argument parsing, dispatch
planner/compiler.py          — Compile orchestration loop
planner/io.py                — Atomic file writes, overwrite logic
planner/openai_client.py     — OpenAI Responses API client
planner/PLANNER_PROMPT.md    — Planner prompt template
planner/prompt_template.py   — Template loading and rendering
planner/validation.py        — Structural + chain validation

utils/run_work_orders.sh     — Batch WO execution shell script
utils/score_work_orders.py   — Plan quality scoring tool
```

## Appendix B: Exit Code Summary

| Component | Code | Meaning |
|---|---|---|
| Factory | `0` | PASS — all acceptance commands succeeded |
| Factory | `1` | FAIL — validation error, preflight failure, or all attempts exhausted |
| Factory | `2` | Unhandled exception in graph |
| Factory | `130` | KeyboardInterrupt (SIGINT) |
| Planner | `0` | Success — work orders written |
| Planner | `1` | General error (missing file, bad args, no subcommand) |
| Planner | `2` | Validation errors in final output |
| Planner | `3` | API / network error |
| Planner | `4` | JSON parse error in LLM output (all attempts) |
| Shell script | `0` | All WOs passed |
| Shell script | `1` | Arg error, missing dir, or first WO failure |
| Score script | `0` | Always (no error exits) |

## Appendix C: Environment Variable Summary

| Variable | Required | Used by | Location |
|---|---|---|---|
| `OPENAI_API_KEY` | Yes | Both planner and factory | `factory/llm.py:15`, `planner/openai_client.py:75` |

No other environment variables are read by production code.
`OPENAI_API_BASE`, `OPENAI_ORG_ID`, and similar are NOT PRESENT.
